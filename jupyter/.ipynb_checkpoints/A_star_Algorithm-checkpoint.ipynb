{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import math\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "from operator import itemgetter\n",
    "import os\n",
    "%matplotlib inline\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of observation\n",
    "num_obs = 100\n",
    "\n",
    "# number of alternatives\n",
    "alternatives = [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "# norm features of the model which are individual for each mode of transportation and used for the calculation of the uc\n",
    "norm_mod_features = [\"norm_travel_time\", \"norm_discomfort\", \"norm_price\"]\n",
    "\n",
    "# norm features of the model which are independet of the mode of transportation and used for the calculation of the uc\n",
    "norm_features = [\"norm_co2\"]\n",
    "\n",
    "# not normalized value of the features dependent of modality: used for initialization of normalized values\n",
    "nnm_features = [\"travel_time\", \"discomfort\", \"price\"]\n",
    "\n",
    "# not normalized value of the features independent of modality: used for initialization of none normalized values\n",
    "nn_features = [\"co2\"]\n",
    "\n",
    "# all mode of transportations\n",
    "mode = [\"car\", \"bike\", \"walk\", \"public\"]\n",
    "\n",
    "# all beta_features applying to normalized features\n",
    "beta_features = [\"norm_co2\", \"norm_num_trans\",\n",
    "                 \"norm_travel_time_car\", \"norm_travel_time_bike\", \"norm_travel_time_walk\", \"norm_travel_time_public\",\n",
    "                 \"norm_discomfort_car\", \"norm_discomfort_bike\", \"norm_discomfort_walk\", \"norm_discomfort_public\",\n",
    "                 \"norm_price_car\", \"norm_price_bike\", \"norm_price_walk\", \"norm_price_public\"]\n",
    "\n",
    "# used for getting all true values\n",
    "all_nn_features = [\"co2\", \"num_trans\",\n",
    "                   \"travel_time_car\", \"travel_time_bike\", \"travel_time_walk\", \"travel_time_public\",\n",
    "                   \"discomfort_car\", \"discomfort_bike\", \"discomfort_walk\", \"discomfort_public\",\n",
    "                   \"price_car\", \"price_bike\", \"price_walk\", \"price_public\"]\n",
    "\n",
    "# only used for efficient purposes in the dijkstra algorithm\n",
    "tm_mt_pc_nt = [\"trans_mode\", \"max_trans\", \"path_cost\", \"num_trans\"]\n",
    "\n",
    "mode_dict = {\"car\": [\"norm_travel_time_car\", \"norm_discomfort_car\", \"norm_price_car\"],\n",
    "            \"bike\": [\"norm_travel_time_bike\", \"norm_discomfort_bike\", \"norm_price_bike\"],\n",
    "            \"walk\": [\"norm_travel_time_walk\", \"norm_discomfort_walk\", \"norm_price_walk\"],\n",
    "            \"public\": [\"norm_travel_time_public\", \"norm_discomfort_public\", \"norm_price_public\"]}\n",
    "\n",
    "\n",
    "nn_mode_dict = {\"car\": [\"travel_time_car\", \"discomfort_car\", \"price_car\"],\n",
    "            \"bike\": [\"travel_time_bike\", \"discomfort_bike\", \"price_bike\"],\n",
    "            \"walk\": [\"travel_time_walk\", \"discomfort_walk\", \"price_walk\"],\n",
    "            \"public\": [\"travel_time_public\", \"discomfort_public\", \"price_public\"]}\n",
    "\n",
    "# if one mode is selected 9 mode attributes remain\n",
    "mode_zeroes = [0.0] * 9\n",
    "\n",
    "# file path for preprocessed dataframes\n",
    "try:\n",
    "    from src.a_star import prep_dataframes  #note: MMWPF path must be added to enviornmental variables PYTHONPATH (see setup.py)\n",
    "    prep_path = prep_dataframes.path()\n",
    "except:\n",
    "    prep_path = os.path.join(\"./\", \"prep_dataframes\")\n",
    "    if not os.path.exists(prep_path):\n",
    "        print('%s path does not exist!' % prep_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lst_g\n",
    "g_file_name = os.path.join(prep_path, \"lst_g\")\n",
    "pickle_in = open(g_file_name,\"rb\")\n",
    "lst_g = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scaler\n",
    "mm_file_name = os.path.join(prep_path, \"min_max\")\n",
    "pickle_in = open(mm_file_name,\"rb\")\n",
    "scaler = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load edge and node network\n",
    "node_path = os.path.join(prep_path, \"all_nodes.pkl\")\n",
    "edge_path = os.path.join(prep_path, \"all_edges.pkl\")\n",
    "\n",
    "all_nodes = pd.read_pickle(node_path)\n",
    "all_edges = pd.read_pickle(edge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\BladeRunner\\\\Desktop_Virtual\\\\BladeRunnerGIT\\\\MMWPF\\\\src\\\\a_star\\\\prep_dataframes\\\\trg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-53c9834636d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load selected target nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrg_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"trg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_file_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlst_sel_trg_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpickle_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\BladeRunner\\\\Desktop_Virtual\\\\BladeRunnerGIT\\\\MMWPF\\\\src\\\\a_star\\\\prep_dataframes\\\\trg'"
     ]
    }
   ],
   "source": [
    "# load selected target nodes\n",
    "trg_file_name = os.path.join(prep_path, \"trg\")\n",
    "pickle_in = open(trg_file_name,\"rb\")\n",
    "lst_sel_trg_id = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\BladeRunner\\\\Desktop_Virtual\\\\BladeRunnerGIT\\\\MMWPF\\\\src\\\\a_star\\\\prep_dataframes\\\\dist_to_trg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d2655248af43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load precalculated distances for selected target nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdict_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dist_to_trg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlst_dist_to_trg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpickle_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\BladeRunner\\\\Desktop_Virtual\\\\BladeRunnerGIT\\\\MMWPF\\\\src\\\\a_star\\\\prep_dataframes\\\\dist_to_trg'"
     ]
    }
   ],
   "source": [
    "# load precalculated distances for selected target nodes\n",
    "dict_file_name = os.path.join(prep_path, \"dist_to_trg\")\n",
    "pickle_in = open(dict_file_name,\"rb\")\n",
    "lst_dist_to_trg = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_all\n",
    "try:\n",
    "    from src.maps import graphML\n",
    "    g_all_path = graphML.path() + 'G_all.graphml'\n",
    "except:\n",
    "    g_all_path = os.path.join(\"./\", \"comp_graph/G_all.graphml\")\n",
    "    if not os.path.exists(g_all_path):\n",
    "        print('%s path does not exist!' % g_all_path)\n",
    "        \n",
    "G_all = ox.load_graphml(g_all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check files if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the all_cost dictionary\n",
    "# all_edges[\"all_cost\"].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the all_cost dictionary\n",
    "# all_edges[\"true_cost\"].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_nodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "544807 in list(all_edges.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all_nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random beta values using a random integer generator\n",
    "def generate_beta_set(rnd_seed):\n",
    "    np.random.seed(rnd_seed)\n",
    "    beta_lst = np.random.randint(1, 100000, len(beta_features), )\n",
    "    sum_beta = sum(beta_lst)\n",
    "    # normalize the beta values between 0 and 1 and sum up to 1\n",
    "    norm_beta_lst = np.array(beta_lst) / sum_beta\n",
    "    \n",
    "    np.random.seed(rnd_seed)\n",
    "    p = np.random.randint(1, 8)\n",
    "    amplify_beta_lst = np.power(norm_beta_lst, p)\n",
    "    sum_beta = sum(amplify_beta_lst)\n",
    "    norm_beta_lst = np.divide(amplify_beta_lst, sum_beta)\n",
    "    \n",
    "    beta_dict = {}\n",
    "    for count, key in enumerate(beta_features):\n",
    "        beta_dict[key] = norm_beta_lst[count]\n",
    "    return beta_dict, norm_beta_lst\n",
    "\n",
    "def generate_beta_samples(num_iter):\n",
    "    beta_samples = []\n",
    "    beta_matrix = []\n",
    "#     rnd_seeds = []\n",
    "    offset = 6 # user_preference can have seed 0 to 9\n",
    "    noise = num_iter * 10\n",
    "    for i in range(len(alternatives)):\n",
    "        # set random seeds for replicable results\n",
    "        rnd_seed = noise + offset + i\n",
    "#         rnd_seeds.append(rnd_seed)\n",
    "        beta_dict, norm_beta_lst = generate_beta_set(rnd_seed)\n",
    "        beta_matrix.append(norm_beta_lst)\n",
    "        beta_samples.append(beta_dict)\n",
    "    return beta_samples, beta_matrix\n",
    "\n",
    "def generate_user_pref():\n",
    "    # random seeds value from 0 to length of the beta_features\n",
    "    # --> makes sure different random seeds are used for path generation\n",
    "    user_pref = generate_beta_set(1)\n",
    "    return user_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_user_pref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_beta_samples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions for the A* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_discomfort(length, mode):\n",
    "    d = length\n",
    "    if mode == \"car\":\n",
    "        d = length\n",
    "    elif mode == \"bike\":\n",
    "        d = 2.0 * length\n",
    "    elif mode == \"walk\":\n",
    "        d = 3.0 * length\n",
    "    elif mode == \"public\":\n",
    "        d = 1.2 * length  \n",
    "    return d\n",
    "\n",
    "def calc_co2(length, mode):\n",
    "    # ref --> 127g / km\n",
    "    if mode == \"car\":\n",
    "        co2 = length * 127 / 1000\n",
    "    # 21g / km\n",
    "    elif mode == \"bike\":\n",
    "        co2 = length * 21 / 1000\n",
    "    # 5g / km\n",
    "    elif mode == \"walk\":\n",
    "        co2 = length * 5 / 1000\n",
    "    # bus = 75, ubahn = 30.5, train = 28 --> avg = 44.5 / km\n",
    "    elif mode == \"public\":\n",
    "        co2 = length * 44.5 / 1000 \n",
    "    return co2\n",
    "\n",
    "# public is calculated in a lambda function since it uses length instead of travel_time\n",
    "def calc_price(travel_time, mode):\n",
    "    # ref --> sixt share start at 0.09 per minute, depends on offer/demand --> assume in average a cost of 0.12 per minute\n",
    "    if mode == \"car\":\n",
    "        pr_cost = 0.002\n",
    "    elif mode == \"bike\":\n",
    "        pr_cost = 0.0013 \n",
    "    elif mode == \"walk\":\n",
    "        pr_cost = 0\n",
    "    # assume 10 % of car cost\n",
    "    elif mode == \"public\":\n",
    "        pr_cost = 0.0004\n",
    "    # opportunity cost of 8€ / hour\n",
    "    op_cost = 8 / 3600\n",
    "    price = (op_cost + pr_cost) * travel_time\n",
    "    return price\n",
    "\n",
    "# order of sel_beta_values needs to be:\n",
    "# 1. co2\n",
    "# 2. travel_time\n",
    "# 3. discomfort\n",
    "# 4. price\n",
    "def calc_heuristic(mode, sel_beta_values, heu_dist_to_trg, spd):\n",
    "    # calculate travel_time with not normalized value first\n",
    "    travel_time = heu_dist_to_trg / spd\n",
    "    co2 = calc_co2(heu_dist_to_trg, mode)\n",
    "    discomfort = calc_discomfort(heu_dist_to_trg, mode)\n",
    "    price = calc_price(travel_time, mode)\n",
    "    arr_cost = np.array([co2, travel_time, discomfort, price]).reshape(1,-1)\n",
    "    arr_cost = scaler.transform(arr_cost)[0]\n",
    "    heuristic = sum([c*b for c, b in zip(arr_cost, sel_beta_values)])\n",
    "    return heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_dict = {\"car\": [\"norm_co2\", \"norm_travel_time_car\", \"norm_discomfort_car\", \"norm_price_car\"],\n",
    "          \"bike\": [\"norm_co2\", \"norm_travel_time_bike\", \"norm_discomfort_bike\", \"norm_price_bike\"],\n",
    "          \"walk\": [\"norm_co2\", \"norm_travel_time_walk\", \"norm_discomfort_walk\", \"norm_price_walk\"],\n",
    "          \"public\": [\"norm_co2\", \"norm_travel_time_public\", \"norm_discomfort_public\", \"norm_price_public\"]}\n",
    "\n",
    "############# getter functions #############\n",
    "def get_node(df_nodes, node_id):\n",
    "    return df_nodes[df_nodes[\"osmid\"] == node_id]\n",
    "\n",
    "def get_edge(df_edges, edge_id):\n",
    "    return df_edges[df_edges[\"edge_id\"] == edge_id]\n",
    "\n",
    "def get_f_cost(node, node_id):\n",
    "    att = \"f_cost\"\n",
    "    return node.loc[node_id, att]\n",
    "\n",
    "def get_explored(node, node_id):\n",
    "    att = \"explored\"\n",
    "    return node.loc[node_id, att]\n",
    "\n",
    "def get_frontier(node, node_id):\n",
    "    att = \"frontier\"\n",
    "    return node.loc[node_id, att]\n",
    "\n",
    "def get_ct_tm_mt_pc_nt(node, node_id):\n",
    "    return node.loc[node_id, tm_mt_pc_nt]\n",
    "    \n",
    "############# setter functions #############\n",
    "def set_explored(node_id, df_nodes):\n",
    "    att = \"explored\"\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, att] = True\n",
    "    \n",
    "def set_frontier(node_id, df_nodes):\n",
    "    att = \"frontier\"\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, att] = True\n",
    "\n",
    "def set_trans_mode(node_id, df_nodes, mode):\n",
    "    att = \"trans_mode\"\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, att] = mode\n",
    "    \n",
    "def set_f_cost(node_id, df_nodes, cost):\n",
    "    att = \"f_cost\"\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, att] = cost\n",
    "    \n",
    "def set_path_cost(node_id, df_nodes, cost):\n",
    "    att = \"path_cost\"\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, att] = cost\n",
    "    \n",
    "############# functions related to updating features after a node is added to the frontier/updated in the frontier #############\n",
    "# calculates the current normalized cost of the node plus the edge cost to the target node\n",
    "def get_all_edge_cost(node_id, node, edge, chosen_mode, ec_norm_num_trans):\n",
    "    # get all the attributes of the not selected modes and append it to the feature names\n",
    "    not_sel_modes = [mode_dict[m] for m in mode if m != chosen_mode]\n",
    "    not_sel_modes = [item for sublist in not_sel_modes for item in sublist]\n",
    "    # for all feature which are dependent on the mode, only the pick the costs for the chosen_mode e.g. travel_time_car\n",
    "    all_norm_features = [f + \"_\" + chosen_mode for f in norm_mod_features]\n",
    "    all_norm_features = norm_features + all_norm_features + [\"norm_num_trans\"] + not_sel_modes\n",
    "    # get the cost for each feature for the current node\n",
    "    lst_node_norm_cost = node.loc[node_id, all_norm_features]\n",
    "    # get the cost for the chosen chosen_mode for each feature for the current edge\n",
    "    lst_edge_norm_cost = list(edge[\"all_cost\"].iloc[0][chosen_mode].values())\n",
    "    # add ec_norm_num_trans to the list of edge cost\n",
    "    # the edge cost for the not selected modes are zero and is appended to it\n",
    "    lst_edge_norm_cost.extend([ec_norm_num_trans] + mode_zeroes)\n",
    "    # add the edge cost to the node cost\n",
    "    lst_final_norm_cost = [n_cost + e_cost for n_cost, e_cost in zip(lst_node_norm_cost, lst_edge_norm_cost)]\n",
    "    return lst_final_norm_cost, all_norm_features, not_sel_modes\n",
    "\n",
    "# not normalized version\n",
    "def get_all_nn_edge_cost(node_id, node, edge, chosen_mode, ec_num_trans):\n",
    "    # get all the attributes of the not selected modes and append it to the feature names\n",
    "    not_sel_modes = [nn_mode_dict[m] for m in mode if m != chosen_mode]\n",
    "    not_sel_modes = [item for sublist in not_sel_modes for item in sublist]\n",
    "    # for all feature which are dependent on the mode, only the pick the costs for the chosen_mode e.g. travel_time_car\n",
    "    all_nn_features = [f + \"_\" + chosen_mode for f in nnm_features]\n",
    "    all_nn_features = nn_features + all_nn_features + [\"num_trans\"] + not_sel_modes\n",
    "    # get the cost for each feature for the current node\n",
    "    lst_node_nn_cost = node.loc[node_id, all_nn_features]\n",
    "    # get the cost for each feature for the current edge\n",
    "    lst_edge_nn_cost = list(edge[\"true_cost\"].iloc[0][chosen_mode].values())\n",
    "    lst_edge_nn_cost.extend([ec_num_trans] + mode_zeroes)\n",
    "    # add the edge cost to the node cost\n",
    "    lst_final_nn_cost = [n_cost + e_cost for n_cost, e_cost in zip(lst_node_nn_cost, lst_edge_nn_cost)]\n",
    "    return lst_final_nn_cost, all_nn_features\n",
    "\n",
    "# set all values of the node except the not normalized cost features \n",
    "def set_all_norm_values(node_id, df_nodes, lst_all_values, all_norm_features_names):\n",
    "    lst_all_features = all_norm_features_names + [\"f_cost\", \"path_cost\", \"previous\", \"edge_idx\", \"trans_mode\", \"max_trans\"]\n",
    "#     print(\"values\", len(lst_all_values))\n",
    "#     print(\"names\", len(lst_all_features))\n",
    "#     print(lst_all_features)\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, lst_all_features] = lst_all_values\n",
    "    \n",
    "def set_all_nn_cost(node_id, df_nodes, lst_cost, all_nn_features_names):\n",
    "    df_nodes.loc[df_nodes[\"osmid\"] == node_id, all_nn_features_names] = lst_cost\n",
    "       \n",
    "        \n",
    "############# functions related to utility cost calculations #############\n",
    "# calculates the path cost for each edge\n",
    "# utiliy function: uses utily cost as the path cost. utily cost = sum of each category multiplied by user preference\n",
    "def get_uc(ct_num_trans, ct_trans_mode, max_trans, edge_id, df_edges, beta_values, heu_dist_to_trg):\n",
    "    edge = get_edge(df_edges, edge_id)\n",
    "    min_cost = sys.maxsize\n",
    "    all_cost_dict = edge[\"all_cost\"].iloc[0]\n",
    "    spd_dict = edge[\"speed_ms\"].iloc[0]\n",
    "    # number of available modalities\n",
    "    num_mod = len(all_cost_dict.keys())\n",
    "    for mode, costs in all_cost_dict.items():  \n",
    "        ########### num_trans calculation ###########\n",
    "        num_trans = ct_num_trans\n",
    "        # num_trans increases by one if the next mode is different form the current mode of transportation\n",
    "        if ct_trans_mode != mode:\n",
    "            num_trans += 1.0\n",
    "        \n",
    "        # prevent dividing by zero\n",
    "        # set num_trans to 0 since in the first iteration from the src_node to the next depth is not counted as a transfer\n",
    "        # max_trans is only zero for the first iteration\n",
    "        if max_trans == 0.0:\n",
    "            # ec stands for edge cost\n",
    "            num_trans = 0.0\n",
    "            ec_num_trans = 0.0\n",
    "            ec_norm_num_trans = 0.0\n",
    "        else:\n",
    "            # the edge cost of num_trans can either be 1 if a transfer is made or 0 if not\n",
    "            ec_num_trans = num_trans - ct_num_trans\n",
    "            # normalize the num_trans by dividing through the maximum number of transfer\n",
    "            ec_norm_num_trans = -(ec_num_trans / max_trans)\n",
    "\n",
    "        if num_trans > max_trans:\n",
    "            print(f\"num_trans: {num_trans} > max_trans: {max_trans}\")\n",
    "         ########### num_trans calculation ###########\n",
    "        \n",
    "        sel_spd = spd_dict[mode]\n",
    "        sel_mode_feat = uc_dict[mode]\n",
    "        sel_beta_values = [beta_values[sel_feat] for sel_feat in sel_mode_feat]\n",
    "        # actual path cost\n",
    "        act_pc = sum([c*b for c,b in zip(list(costs.values()), sel_beta_values)]) + beta_values[\"norm_num_trans\"] * ec_norm_num_trans\n",
    "        act_pc = abs(act_pc)\n",
    "        heu_cost = calc_heuristic(mode, sel_beta_values, heu_dist_to_trg, sel_spd)\n",
    "        f_cost = act_pc + abs(heu_cost)\n",
    "\n",
    "        abs_cost = abs(f_cost)\n",
    "        if abs_cost < min_cost:\n",
    "            min_cost = abs_cost\n",
    "            min_mode = mode\n",
    "            # only the difference is added to the current num_trans cost\n",
    "            # e.g. after reaching the target node num_trans=7 and ct_num_trans=6, then aad 7-6=1 to it\n",
    "            min_ec_num_trans = ec_num_trans\n",
    "            if ec_num_trans > 1:\n",
    "                print(\"Something went wrong\", ec_num_trans)\n",
    "            min_ec_norm_num_trans = ec_norm_num_trans\n",
    "            min_act_pc = act_pc\n",
    "#     print(f\"ec_num_trans={ec_num_trans}, ec_norm_num_trans={ec_norm_num_trans}, ct_num_trans={ct_num_trans}, num_trans={num_trans}, max_trans={max_trans}\")\n",
    "    return min_cost, min_act_pc, min_mode, min_ec_num_trans, min_ec_norm_num_trans, edge\n",
    "\n",
    "# calculate the utility cost for the specified path alternative\n",
    "# and return all feature values of the alternative as a dict\n",
    "def calc_uc_with_user_pref(df_nodes, target_id, user_pref):\n",
    "    node = get_node(df_nodes, target_id)\n",
    "    utility_cost = 0\n",
    "    feature_values = []\n",
    "    #  key == feature e.g. travel_time_car\n",
    "    for key, beta_value in user_pref.items():\n",
    "        f_value = node.loc[target_id, key]\n",
    "        utility_cost = f_value * beta_value + utility_cost\n",
    "        feature_values.append(f_value)\n",
    "    # dict containing final value of each feature\n",
    "    d = dict(zip(beta_features, feature_values))\n",
    "    return [abs(utility_cost), d], feature_values\n",
    "\n",
    "    \n",
    "############# Functions related to route calculation #############\n",
    "def get_route(df_nodes, target_id):\n",
    "    lst = []    \n",
    "    calculate_route(df_nodes, target_id, lst)\n",
    "    lst.reverse()\n",
    "    return lst\n",
    "\n",
    "def calculate_route(df_nodes, target_id, lst):\n",
    "    lst.append(target_id)\n",
    "    att = \"previous\"\n",
    "    previous_node_id = get_node(df_nodes, target_id)[att].iloc[0]\n",
    "    if previous_node_id is None:\n",
    "        return\n",
    "    calculate_route(df_nodes, previous_node_id, lst)    \n",
    "\n",
    "############# Addtional functions #############\n",
    "def get_all_nn_values(df_nodes, target_id):\n",
    "    node = get_node(df_nodes, target_id)\n",
    "    feature_values = node.loc[target_id, all_nn_features]\n",
    "    return feature_values\n",
    "\n",
    "# initiate all nodes with maximum pathcost, not explored and no previous node\n",
    "def initiate_nodes(df_nodes):\n",
    "    df_nodes[\"f_cost\"] = sys.maxsize\n",
    "    df_nodes[\"path_cost\"] = 0.0\n",
    "    df_nodes[\"heuristic\"] = 0.0\n",
    "    df_nodes[\"num_trans\"] = 0.0\n",
    "    df_nodes[\"norm_num_trans\"] = 0.0\n",
    "    df_nodes[\"max_trans\"] = 0.0\n",
    "    df_nodes[\"previous\"] = None\n",
    "    df_nodes[\"explored\"] = False\n",
    "    df_nodes[\"frontier\"] = False\n",
    "    df_nodes[\"trans_mode\"] = None\n",
    "    df_nodes[\"edge_idx\"] = None\n",
    "    for f in nnm_features:\n",
    "        for m in mode:\n",
    "            df_nodes[\"norm\" + \"_\" + f + \"_\" + m] = 0.0\n",
    "            df_nodes[f + \"_\" + m] = 0.0       \n",
    "    for nn_f in nn_features:\n",
    "        df_nodes[\"norm\" + \"_\" + nn_f] = 0.0\n",
    "        df_nodes[nn_f] = 0.0\n",
    "\n",
    "############# Not used right now #############\n",
    "def print_path_costs(df_nodes, trg_id):\n",
    "    for alt in alternatives:\n",
    "        print(alt, \":\", round(get_path_cost(df_nodes, trg_id, alt), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that implements Dijkstra's single source shortest path algorithm for a graph  \n",
    "# src_id: osmid of the starting node\n",
    "# target_id: osmid of the target ndode\n",
    "# df_nodes: node dataframe to manipulate\n",
    "# df_edges: edge dataframe to get data from\n",
    "# beta_values: beta_values for the utility function\n",
    "\n",
    "def astar(src_id: int, target_id: int, df_nodes, df_edges, beta_values: dict, dist_to_trg: dict):\n",
    "    \n",
    "    # Set the cost for the start node to zero \n",
    "    start = get_node(df_nodes, src_id)\n",
    "    set_f_cost(src_id, df_nodes, 0)\n",
    "    set_path_cost(src_id, df_nodes, 0)\n",
    "    set_trans_mode(src_id, df_nodes, \"all\")\n",
    "\n",
    "    # used as a priority queue and for quickly updating the cost\n",
    "    # initialized with the the src_node\n",
    "    # src_id : path_cost\n",
    "    frontier = {src_id: get_f_cost(get_node(df_nodes, src_id), src_id)}\n",
    "    # max_num_trans is the maximum depth across all expanded nodes - 1. \n",
    "    max_trans= 0\n",
    "    \n",
    "    while len(frontier):\n",
    "        ct_node_id = next(iter(frontier))\n",
    "        # Pops a vertex with the lowest cost\n",
    "        frontier.pop(ct_node_id)\n",
    "        ct_node = get_node(df_nodes, ct_node_id)\n",
    "        # get all feature of the current node\n",
    "        # ct_max trans is the current depth of the node -1 since the first iteration is not counted as a transfer\n",
    "        ct_trans_mode, ct_max_trans, ct_path_cost, ct_num_trans = get_ct_tm_mt_pc_nt(df_nodes, ct_node_id)\n",
    "        \n",
    "        # compare current depth and the the max depth across all the expanded nodes so far\n",
    "        if src_id != ct_node_id:\n",
    "            # Add +1 since it already accounts for the depth of the target node but only if it is not the first iteration.\n",
    "            # Since in the first iteration from the src_node to the next depth is not counted as a transfer.\n",
    "            ct_max_trans += 1\n",
    "            if ct_max_trans > max_trans:\n",
    "                max_trans = ct_max_trans\n",
    "\n",
    "        # stop if the destination has been reached\n",
    "        if ct_node_id == target_id:\n",
    "            break\n",
    "\n",
    "        # set the the current node to explored = True\n",
    "        set_explored(ct_node_id, df_nodes)\n",
    "        \n",
    "        # skip the node if it does not have any adjacent nodes since this is a dead end if it is not the target node\n",
    "        # if the adjacent value is a float type it means it contains a nan value\n",
    "        if type(ct_node[\"adjacent\"].iloc[0]) == float:\n",
    "            print(ct_node_id, \"has not adjacent nodes\")\n",
    "            continue\n",
    "\n",
    "        # n returns a tuple (start_node_id, adjacent_node_id, key)\n",
    "        # or a list of tuple if there are multiple edges from the start to the same target node\n",
    "        for edge_id in ct_node[\"adjacent\"].iloc[0]:\n",
    "            # edge_id = [(1,2,0), ..., (1,2,1)] --> edge_id[0][1] = target node id \n",
    "            if type(edge_id) == list:\n",
    "                node_id = edge_id[0][1]   \n",
    "            else:\n",
    "                node_id = edge_id[1]\n",
    "\n",
    "            # child node\n",
    "            node = get_node(df_nodes, node_id)\n",
    "            # get the heuristic distance to target\n",
    "            heu_dist_to_trg = dist_to_trg[node_id]\n",
    "\n",
    "            # if visited, skip\n",
    "            if get_explored(node, node_id):\n",
    "                continue\n",
    "\n",
    "            # if edge_id is a list it means there are multiple edges from the start node u to the same target node v\n",
    "            if type(edge_id) == list:\n",
    "                # convert into a list of tuples consisting of (min_cost, min_mode, edge_id)\n",
    "                # and sort them in an ascending order so that the first element is the edge with the min_cost\n",
    "                # example cost_tuple: ((0.010230682933306881, 'walk'), (7443344681, 7443344682, 0))\n",
    "                # e.g. e_cost, e_mode, ec_num_trans, ec_norm_num_trans = get_uc(ct_num_trans, ct_trans_mode, max_trans, edge, beta_values)\n",
    "                cost_lst = sorted([get_uc(ct_num_trans, ct_trans_mode, max_trans, e, df_edges, beta_values, heu_dist_to_trg) for e in edge_id], key=itemgetter(0))\n",
    "#                 print(\"Edge_id == List:\", [c[0] for c in cost_lst])\n",
    "                min_lst = cost_lst[0]\n",
    "#                 print(min_lst)\n",
    "                min_f_cost, min_act_pc, mode, min_ec_num_trans, min_ec_norm_num_trans, edge = min_lst[:]\n",
    "                new_f_cost = ct_path_cost + min_f_cost\n",
    "            else:\n",
    "                min_f_cost, min_act_pc, mode, min_ec_num_trans, min_ec_norm_num_trans, edge = get_uc(ct_num_trans, ct_trans_mode, max_trans, edge_id,\n",
    "                                                                                                     df_edges, beta_values, heu_dist_to_trg)\n",
    "                new_f_cost = ct_path_cost + min_f_cost\n",
    "            \n",
    "            # the first time nodes are discovered as child nodes, their new_cost will always be lower \n",
    "            # than the initial cost = np.inf \n",
    "            if new_f_cost < get_f_cost(node, node_id):\n",
    "                # update all mode dependent features to the values of the current node\n",
    "                # Reason for that is the edge cost only updates the selected mode of transportation\n",
    "#                 update_trg_node(df_nodes, ct_node_id, node_id)\n",
    "#                 update_nn_trg_node(df_nodes, ct_node_id, node_id)\n",
    "                new_path_cost = ct_path_cost + min_act_pc\n",
    "\n",
    "                lst_all_norm_cost, all_norm_features_names, not_sel_modes = get_all_edge_cost(ct_node_id, ct_node,\n",
    "                                                                                              edge, mode,\n",
    "                                                                                              min_ec_norm_num_trans)\n",
    "                lst_all_nn_cost, all_nn_features_names = get_all_nn_edge_cost(ct_node_id, ct_node, edge, mode,\n",
    "                                                                              min_ec_num_trans)\n",
    "\n",
    "#                 print(lst_all_norm_cost)\n",
    "                lst_all_values = lst_all_norm_cost + [new_f_cost, new_path_cost, ct_node_id, edge.index[0], mode, ct_max_trans]\n",
    "                set_all_norm_values(node_id, df_nodes, lst_all_values, all_norm_features_names)\n",
    "                set_all_nn_cost(node_id, df_nodes, lst_all_nn_cost, all_nn_features_names)\n",
    "                \n",
    "                # Add the child node_id to the frontier if it is not in the set\n",
    "                # If node is already in the frontier the cost of the node will be updated to the lower cost\n",
    "                frontier[node_id] = new_f_cost\n",
    "                    \n",
    "#                 print(\"updated : current = %s next = %s new_cost = %s\" \n",
    "#                       %(current_node_id, node_id, min_cost))\n",
    "#             else:\n",
    "#                 print(\"not updated : current = %s next = %s new_cost = %s\" \n",
    "#                       %(current_node_id, node_id, min_cost))\n",
    "\n",
    "        # Resort the frontier\n",
    "        frontier = {k: v for k, v in sorted(frontier.items(), key=itemgetter(1))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for generating observations and alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_path_alt(src_id, trg_id, df_nodes, df_edges, beta_samples, user_pref, dist_to_trg):\n",
    "    # run the dijkstra algorithm over all alternatives\n",
    "    uc_cost_lst = []\n",
    "    exp_var_matrix = []\n",
    "    nn_exp_var_matrix = []\n",
    "    routes = []\n",
    "    uc_cost_only_lst = []\n",
    "#     df_node_lst = []\n",
    "#     df_edge_lst = []\n",
    "    for i in range(len(alternatives)):\n",
    "        print(f\"\\n Starting with {alternatives[i]}\")\n",
    "        astar(src_id, trg_id, df_nodes, df_edges, beta_samples[i], dist_to_trg)\n",
    "        uc_dic_lst, exp_var_lst = calc_uc_with_user_pref(df_nodes, trg_id, user_pref)\n",
    "        nn_exp_var_lst = get_all_nn_values(df_nodes, trg_id)\n",
    "        nn_exp_var_matrix.append(nn_exp_var_lst)\n",
    "        uc_cost_only_lst.append(uc_dic_lst[0])\n",
    "        uc_dic_lst.insert(0, alternatives[i])\n",
    "        uc_cost_lst.append(uc_dic_lst)\n",
    "        exp_var_matrix.append(exp_var_lst)\n",
    "        routes.append(get_route(df_nodes, trg_id))\n",
    "        ########### Uncomment #######################\n",
    "#         df_node_lst.append(df_nodes.copy())\n",
    "#         df_edge_lst.append(df_edge_lst.copy())\n",
    "        ########### For looking into the df #########\n",
    "        \n",
    "        # reset the df_nodes dataframe before the the next iteration\n",
    "        initiate_nodes(df_nodes)\n",
    "        \n",
    "    # get the index (same as the # of the alternative) with the minimum utility cost\n",
    "    best_alt_idx = uc_cost_only_lst.index(min(uc_cost_only_lst))\n",
    "    # set the index with the minimum utility cost to 1 and else 0\n",
    "    choice_vector = [1 if i == best_alt_idx else 0 for i in range(len(uc_cost_only_lst))]\n",
    "    return uc_cost_lst, routes, exp_var_matrix, nn_exp_var_matrix, choice_vector, uc_cost_only_lst #, df_node_lst, df_edge_lst\n",
    "\n",
    "# lst_id: list of source and target id tuple\n",
    "def generate_stated_pref_survey(lst_id, df_nodes, df_edges, lst_dist_to_trg):\n",
    "    # file path for storing data\n",
    "    try:\n",
    "        from src.a_star import data  #note: MMWPF path must be added to enviornmental variables PYTHONPATH (see setup.py)\n",
    "        data_path = data.path()\n",
    "    except:\n",
    "        data_path = os.path.join(\"./\", \"data\")\n",
    "        if not os.path.exists(data_path):\n",
    "            print('%s path does not exist, creating folder!' % data_path)\n",
    "            os.makedirs(data_path)\n",
    "            \n",
    "    exp_file_name = os.path.join(data_path, \"exp_var\")\n",
    "    user_file_name = os.path.join(data_path, \"user_pref\")\n",
    "    route_file_name = os.path.join(data_path, \"route\")\n",
    "    beta_file_name = os.path.join(data_path, \"beta\")\n",
    "    lst_file_names = [exp_file_name, route_file_name, beta_file_name]\n",
    "    user_pref, user_matrix = generate_user_pref()\n",
    "    all_beta_matrix = []\n",
    "    all_exp_var_matrix = []\n",
    "    all_nn_exp_var_matrix = []\n",
    "    uc_cost_only_matrix = []\n",
    "    all_choice_matrix = []\n",
    "    routes_matrix = []\n",
    "#     df_n_mat = []\n",
    "#     df_e_mat = []\n",
    "    # list of tuples of selected alternatives and all the other alternatives\n",
    "    lst = []\n",
    "    \n",
    "    try: \n",
    "        data_file = open(user_file_name, 'wb')\n",
    "        pickle.dump(user_matrix, data_file)\n",
    "        data_file.close() \n",
    "    except: \n",
    "        print(\"Something went wrong\")\n",
    "    \n",
    "    # use different beta_values in each iteration to get a diverse set of paths\n",
    "    for count, node_tuple in enumerate(lst_id):\n",
    "        src_id, trg_id = node_tuple[0], node_tuple[1]\n",
    "        clear_output(wait=True)\n",
    "        print(f\"\\n Iteration {count}: From src {src_id} to trg {trg_id}\")\n",
    "        beta_samples, beta_matrix = generate_beta_samples(count)\n",
    "        # add df_node_lst, df_edge_lst after uc_cost_only if adding the dataframes\n",
    "        uc_cost_lst, routes, exp_var_matrix, nn_exp_var_matrix, choice_vector, uc_cost_only = all_path_alt(src_id, trg_id, \n",
    "                                                                                                          df_nodes, df_edges, \n",
    "                                                                                                          beta_samples, user_pref,\n",
    "                                                                                                          lst_dist_to_trg[count] \n",
    "                                                                                                          )\n",
    "        all_beta_matrix.extend(beta_matrix)\n",
    "        all_exp_var_matrix.extend(exp_var_matrix)\n",
    "        all_nn_exp_var_matrix.extend(nn_exp_var_matrix)\n",
    "        all_choice_matrix.extend(choice_vector)\n",
    "        routes_matrix.append(routes)\n",
    "        uc_cost_only_matrix.append(uc_cost_only)\n",
    "        lst.append(uc_cost_lst)\n",
    "        ########### Uncomment #######################\n",
    "#         df_n_mat.append(df_node_lst)\n",
    "#         df_e_mat.append(df_edge_lst)\n",
    "        ########### For looking into the df #########\n",
    "        # after each iteration save data as file\n",
    "        for idx, filename in enumerate(lst_file_names):\n",
    "            try: \n",
    "                data_file = open(filename, 'wb')\n",
    "                if idx == 0:\n",
    "                    pickle.dump(all_exp_var_matrix, data_file)\n",
    "                if idx == 1:\n",
    "                    pickle.dump(routes_matrix, data_file)\n",
    "                if idx == 2:\n",
    "                    pickle.dump(all_beta_matrix, data_file)\n",
    "                data_file.close() \n",
    "            except:\n",
    "                print(\"Something went wrong\")\n",
    "    all_beta_matrix = np.array(all_beta_matrix)\n",
    "    all_exp_var_matrix = np.array(all_exp_var_matrix)\n",
    "    print(\"Algorithm finished running\")\n",
    "    return lst, user_matrix, all_beta_matrix, all_exp_var_matrix, all_nn_exp_var_matrix, all_choice_matrix, uc_cost_only_matrix, routes_matrix #,df_n_mat, df_e_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preselect source id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_id preselection\n",
    "# filter all nodes out that have no adjacent nodes\n",
    "ids_no_adj = list(all_nodes[all_nodes['adjacent'].isnull()].osmid)\n",
    "df_filtered = all_nodes[~all_nodes.osmid.isin(ids_no_adj)]\n",
    "# filter all the target nodes out\n",
    "df_filtered = df_filtered[~df_filtered.osmid.isin(lst_sel_trg_id)]\n",
    "lst_all_ids = list(df_filtered.osmid)\n",
    "lst_sel_src_id = []\n",
    "\n",
    "count = 1507012\n",
    "all_has_path = False\n",
    "\n",
    "while len(lst_sel_src_id) < num_obs:\n",
    "    heu_dist = lst_dist_to_trg[len(lst_sel_src_id)]\n",
    "    trg_id = lst_sel_trg_id[len(lst_sel_src_id)]\n",
    "    np.random.seed(count)\n",
    "    src_id = np.random.choice(lst_all_ids, replace=False)\n",
    "    dist_to_trg = heu_dist[src_id]\n",
    "    has_path = nx.has_path(G_all, src_id, trg_id)\n",
    "    dist_25_perc = np.percentile(list(heu_dist.values()), 25)\n",
    "    dist_10_perc = np.percentile(list(heu_dist.values()), 10)\n",
    "    if has_path and dist_10_perc <= dist_to_trg <= dist_25_perc:\n",
    "        lst_sel_src_id.append(src_id)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_id = [(src_id, trg_id) for src_id, trg_id in zip(lst_sel_src_id, lst_sel_trg_id)]\n",
    "lst_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save precalculated lst_id to file\n",
    "sel_id_file_name = os.path.join(prep_path, \"sel_src_trg_ids4_100\")\n",
    "try: \n",
    "    geeky_file = open(sel_id_file_name, 'wb') \n",
    "    pickle.dump(lst_id, geeky_file) \n",
    "    geeky_file.close() \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Source Target (S/T) Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "composed_graph_path = os.path.join(\"./\", \"comp_graph\")\n",
    "\n",
    "# hardcoded path - needs to be adjusted\n",
    "g_public_path = os.path.join(composed_graph_path, \"G_public.graphml\")\n",
    "G_public = ox.load_graphml(g_public_path)\n",
    "\n",
    "# Project graph to 3395 to make CRS coherent with the rest of the objects\n",
    "projected_graph_public = ox.project_graph(G_public, to_crs=\"EPSG:3395\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project graph to 3395 to make CRS coherent with the rest of the objects\n",
    "projected_graph_all = ox.project_graph(G_all, to_crs=\"EPSG:3395\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = ox.plot_graph(projected_graph_public, figsize=(12,6), show=False, close=True, dpi=100);\n",
    "\n",
    "points = []\n",
    "for src, trg in lst_id:\n",
    "    x1 = projected_graph_all.nodes[src]['x']\n",
    "    y1 = projected_graph_all.nodes[src]['y']\n",
    "    x2 = projected_graph_all.nodes[trg]['x']\n",
    "    y2 = projected_graph_all.nodes[trg]['y']\n",
    "    points.append([x1, x2, y1, y2])\n",
    "\n",
    "scatter_list = []\n",
    "line_list = []\n",
    "\n",
    "def animate(i):\n",
    "    clear_output(wait=True)\n",
    "    print('Frame: %d' % i)\n",
    "    ax.plot([points[i][0], points[i][1]], [points[i][2], points[i][3]], c='r')\n",
    "    ax.scatter(points[i][0], points[i][2], s=20, c='y')\n",
    "    ax.scatter(points[i][1], points[i][3], s=20, c='g')\n",
    "    ax.annotate(str(i), ((points[i][0]+points[i][1])/2, (points[i][2]+points[i][3])/2), color='c', weight='bold', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Make the animation\n",
    "animation = FuncAnimation(fig, animate, frames=len(lst_id))   \n",
    "\n",
    "# to display animation in Jupyter Notebook\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run astar algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the notebook here before manually starting the dijkstra algorithm\n",
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify set to test\n",
    "# start_idx = 0\n",
    "# end_idx = 1\n",
    "# sample_lst_id = lst_id[start_idx:end_idx]\n",
    "# sample_lst_dist_to_trg = lst_dist_to_trg[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nodes = all_nodes.copy()\n",
    "df_edges = all_edges.copy()\n",
    "\n",
    "# add , df_node_mat, df_edge_mat if showing dataframes\n",
    "# sp, up, beta_matrix, exp_var_matrix, nn_exp_var_matrix, choice_matrix, uc_cost, routes_matrix, df_node_mat, df_edge_mat= generate_stated_pref_survey(sample_lst_id, df_nodes, df_edges, sample_lst_dist_to_trg)\n",
    "# sp\n",
    "sp, up, beta_matrix, exp_var_matrix, nn_exp_var_matrix, choice_matrix, uc_cost, routes_matrix = generate_stated_pref_survey(lst_id, df_nodes, df_edges, lst_dist_to_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the traversed nodes and their costs\n",
    "# sel_iter = 0\n",
    "# sel_alt = 2\n",
    "# r = routes_matrix[sel_iter][sel_alt]\n",
    "# df_0 = df_node_mat[sel_iter][sel_alt]\n",
    "# df_f = df_0[df_0[\"osmid\"].isin(r)]\n",
    "# pd.set_option(\"display.max_rows\", 71)\n",
    "# df_f[all_nn_features + [\"previous\"]]\n",
    "\n",
    "# all_nn_features / beta_features\n",
    "# 1022442081,\n",
    "#  3397235135,\n",
    "#  513745291,\n",
    "#  363119,\n",
    "#  25231134,\n",
    "#  25231133,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all results into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(arr, names):\n",
    "    df = pd.DataFrame(arr, columns=names)\n",
    "    return df\n",
    "\n",
    "df_beta_matrix = convert_to_df(beta_matrix, beta_features)\n",
    "\n",
    "df_exp_var_matrix = convert_to_df(exp_var_matrix, beta_features)\n",
    "\n",
    "df_nn_exp_var_matrix = convert_to_df(nn_exp_var_matrix, all_nn_features)\n",
    "\n",
    "names = [\"choice\"]\n",
    "re_choice = np.reshape(choice_matrix, (len(choice_matrix), 1))\n",
    "df_choice_matrix = convert_to_df(re_choice, names)\n",
    "\n",
    "names = [\"ALT_1\", \"ALT_2\", \"ALT_3\", \"ALT4\"]\n",
    "df_uc_matrix = convert_to_df(uc_cost, names)\n",
    "\n",
    "routes_matrix\n",
    "names = [\"ALT_1\", \"ALT_2\", \"ALT_3\", \"ALT4\"]\n",
    "df_routes_matrix = convert_to_df(routes_matrix, names)\n",
    "\n",
    "user_pref = np.reshape(up, (1,len(beta_features)))\n",
    "df_user_pref = convert_to_df(user_pref, beta_features)\n",
    "\n",
    "# create correlation matrix\n",
    "df_corr = df_exp_var_matrix.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [df_beta_matrix, df_exp_var_matrix, df_nn_exp_var_matrix, df_choice_matrix, df_uc_matrix, df_routes_matrix, df_user_pref, df_corr]\n",
    "list_df_names = [\"beta_matrix.csv\", \"exp_var_matrix.csv\", \"nn_exp_var_matrix.csv\", \"choice_matrix.csv\", \"uc_matrix.csv\", \"routes_matrix.csv\", \"user_pref.csv\", \"corr.csv\"]\n",
    "for count, df in enumerate(list_df):\n",
    "    df.to_csv(list_df_names[count], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 400)\n",
    "df_exp_var_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nn_exp_var_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_choice_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_uc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes_matrix.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_matrix = pd.read_csv(\"beta_matrix.csv\")\n",
    "df_exp_var_matrix = pd.read_csv(\"exp_var_matrix.csv\")\n",
    "df_true_exp_var_matrix = pd.read_csv(\"true_exp_var_matrix.csv\")\n",
    "df_choice_matrix = pd.read_csv(\"choice_matrix.csv\")\n",
    "df_uc_matrix = pd.read_csv(\"uc_matrix.csv\")\n",
    "df_routes_matrix = pd.read_csv(\"routes_matrix.csv\")\n",
    "df_user_pref = pd.read_csv(\"user_pref.csv\")\n",
    "df_corr = pd.read_csv(\"corr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = 11\n",
    "r = routes_matrix[obs]\n",
    "uc = uc_cost[obs]\n",
    "sorted_route_lst = r\n",
    "# sorted_lst = sorted([[r[i], uc[i]] for i in range(len(uc))], key=lambda x: x[1])\n",
    "# sorted_route_lst =[t[0] for t in sorted_lst]\n",
    "# best_route is always red\n",
    "colors = [\"r\", \"b\", \"m\", \"c\"] # g\", \"y\",\n",
    "col_lst = colors\n",
    "# fig, ax = ox.plot_graph_routes(G_all, sorted_route_lst, col_lst, **{\"route_linewidth\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = ox.plot_graph_routes(G_all, sorted_route_lst, col_lst, **{\"route_linewidth\": 4})\n",
    "fig.set_size_inches(70, 80)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('test2png.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoomed in Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import sys\n",
    "\n",
    "'''run once to get all lat/lon of nodes'''\n",
    "G_all_nodIdLatDict = nx.get_node_attributes(G_all, \"y\")\n",
    "G_all_nodIdLonDict = nx.get_node_attributes(G_all, \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' create graph for area of interest'''\n",
    "#~30 to 120 seconds w/ intel i7 8th gen\n",
    "G_tmp = nx.MultiDiGraph()\n",
    "#get max x,y coordinate from alternatives\n",
    "r_coord_x = []\n",
    "r_coord_y = []\n",
    "for route in r:\n",
    "    r_coord_x = []\n",
    "    r_coord_y = []\n",
    "    for nodeId in route:\n",
    "        r_coord_x.append(G_all_nodIdLonDict[nodeId]) #x\n",
    "        r_coord_y.append(G_all_nodIdLatDict[nodeId]) #y\n",
    "\n",
    "#note: offset=0.001 corresponds to 111.12 meters\n",
    "offset = 0.01\n",
    "latMax = max(r_coord_y) + offset\n",
    "latMin = min(r_coord_y) - offset\n",
    "lonMax = max(r_coord_x) + offset\n",
    "lonMin = min(r_coord_x) - offset\n",
    "\n",
    "print(latMax, latMin)\n",
    "print(lonMax, lonMin)\n",
    "\n",
    "latIdList = [nodeId for nodeId in G_all_nodIdLatDict \n",
    "             if G_all_nodIdLatDict[nodeId] > latMin and G_all_nodIdLatDict[nodeId] < latMax]\n",
    "lonIdList = [nodeId for nodeId in G_all_nodIdLonDict \n",
    "             if G_all_nodIdLonDict[nodeId] > lonMin and G_all_nodIdLonDict[nodeId] < lonMax]\n",
    "\n",
    "print('num_lat_nodes: %d' % len(latIdList))\n",
    "print('num_lon_nodes: %d' % len(lonIdList))\n",
    "\n",
    "if len(lonIdList) < len(latIdList):\n",
    "    nodeList_wAttr = [(nodeId, {'y': G_all_nodIdLatDict[nodeId], 'x': G_all_nodIdLonDict[nodeId]}) \n",
    "                      for nodeId in lonIdList if nodeId in latIdList]\n",
    "else:\n",
    "    nodeList_wAttr = [(nodeId, {'y': G_all_nodIdLatDict[nodeId], 'x': G_all_nodIdLonDict[nodeId]})\n",
    "                 for nodeId in latIdList if nodeId in lonIdList]\n",
    "\n",
    "if nodeList_wAttr:  #if nodeList_wAttr is not empty\n",
    "    G_tmp.add_nodes_from(nodeList_wAttr)\n",
    "\n",
    "try:\n",
    "    nodeList = list(zip(*nodeList_wAttr))[0]\n",
    "    edgeList_wAttr = []\n",
    "    for i in range(0, len(nodeList_wAttr)): #iterate through all the nodes\n",
    "        for v in G_all.edges._adjdict[nodeList_wAttr[i][0]]: #get the edges connected to these nodes\n",
    "            #if v not in nodeList: #if the edge node not in the node list, add it (means its outside of boundaries)\n",
    "                #G_tmp.add_node(v, y=G_all.nodes[v]['y'], x=G_all.nodes[v]['x'])\n",
    "            if v in nodeList:\n",
    "                edgeList_wAttr.append((nodeList_wAttr[i][0], v, G_all.edges._adjdict[nodeList_wAttr[i][0]][v][0]))\n",
    "    G_tmp.add_edges_from(edgeList_wAttr) \n",
    "    \n",
    "except Exception as e:\n",
    "    print('Exception!: %s' % e)\n",
    "\n",
    "\n",
    "G_tmp.graph['crs'] = G_all.graph['crs']                   #copy from G_all\n",
    "G_tmp.graph['created_with'] = G_all.graph['created_with']  \n",
    "G_tmp.graph['simplified'] = G_all.graph['simplified']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create an animation showing routes of selected observation'''\n",
    "#~30 to 60 seconds w/ intel i7 8th gen\n",
    "f, a = plt.subplots(figsize=(16, 15), dpi=300);\n",
    "a.axis('off')\n",
    "\n",
    "def animate(i):\n",
    "    clear_output(wait=True)\n",
    "    print('Frame: %d' % i)\n",
    "    try:\n",
    "        fig, ax = ox.plot_graph_route(G_tmp, sorted_route_lst[i], col_lst[i], route_linewidth=8, \n",
    "                                      figsize=(16, 15), dpi=300, \n",
    "                                      bgcolor='w', show=False, close=True)\n",
    "        canvas = FigureCanvas(fig)\n",
    "#         canvas.draw()       # draw the canvas, cache the renderer\n",
    "        s, (width, height) = canvas.print_to_buffer();\n",
    "        image = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
    "        plt.tight_layout()\n",
    "        a.imshow(image)\n",
    "        plt.close()\n",
    "    except:\n",
    "        print('!!a route may have been cut off, try increasing lat/lon offset!!\\n\\n')\n",
    "        sys.exit()\n",
    "\n",
    "# Make the animation\n",
    "animation = FuncAnimation(f, animate, frames=len(sorted_route_lst));   \n",
    "\n",
    "# to display animation in Jupyter Notebook\n",
    "HTML(animation.to_jshtml()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "titles = []\n",
    "for i in range(0, len(sorted_route_lst)):\n",
    "    clear_output(wait=True)\n",
    "    print('Frame: %d' % i)\n",
    "    \n",
    "    fig1, a = ox.plot_graph_route(G_tmp, sorted_route_lst[i], col_lst[i], route_linewidth=50, \n",
    "                                  figsize=(70, 70), bgcolor='w', show=False, close=True)\n",
    "    ox.plot._config_ax(a, crs=G_tmp.graph['crs'], bbox=(latMax, latMin, lonMax, lonMin), padding=0.0)\n",
    "    \n",
    "    fig1.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    fig1.canvas.draw()\n",
    "    canvas = FigureCanvas(fig1)\n",
    "    \n",
    "    s, (width, height) = canvas.print_to_buffer();\n",
    "    image = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
    "    if i == 0:\n",
    "        fig, ax = plt.subplots(2,2, figsize=(15,15))\n",
    "        fig.tight_layout()\n",
    "    titles.append('Alt: %d from observation: %d' %(i, obs))\n",
    "    ix = np.unravel_index(i, ax.shape) #get ax subplot shape\n",
    "    ax[ix].imshow(image, aspect='auto')\n",
    "    ax[ix].set_title(titles[i], fontsize=20)\n",
    "    ax[ix].set_axis_off()\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    \n",
    "plt.show();\n",
    "\n",
    "fig_path = os.path.join(\"./\", \"fig/\")\n",
    "if not os.path.exists(fig_path):\n",
    "            print('%s path does not exist, creating folder!' % fig_path)\n",
    "            os.makedirs(fig_path)   \n",
    "fig.savefig(fig_path + '4_alternatives.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backup astar with astar cost calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using the built in astar algorithm to calculate the shortest distance to each node\n",
    "# # lst_sel_trg_id: list containing all trg ids\n",
    "# # G_all: Multigraph containing the whole network\n",
    "\n",
    "# # dict of dictionaries containing the distance of each src_node to each target node\n",
    "# dict_dist_to_trg = {}\n",
    "# lst_id = list(nx.get_node_attributes(G_all, \"osmid\").values())\n",
    "\n",
    "# for count, trg_id in enumerate(lst_sel_trg_id):\n",
    "#     clear_output(wait=True)\n",
    "#     dist_to_trg = {}\n",
    "#     # calculate the shortest distance of each src_id to the trg_id\n",
    "#     # it is always underapproximative since it is the shortest distance possible to the trg_id\n",
    "#     for count_src, src_id in enumerate(lst_id):\n",
    "#         try:\n",
    "#             shortest_dist = nx.astar_path_length(G_all, source=src_id, target=trg_id, heuristic=dist_heuristic, weight=\"length\")\n",
    "#         except:\n",
    "#             # if not node is not reachable it assign maximum distance to it, so it will never be picked\n",
    "#             shortest_dist = sys.maxsize\n",
    "#             print(f\"Not reachable from {src_id}, assigned distance: {shortest_dist}\")\n",
    "#         finally:\n",
    "#             dist_to_trg[src_id] = shortest_dist\n",
    "#             print(f\"Remaining number of src_ids: {len(lst_id) - (count_src+1)}\")\n",
    "#     print(f\"Remaining number of trg_ids: {len(lst_sel_trg_id) - (count+1)}\")\n",
    "#     dict_dist_to_trg[trg_id] = dist_to_trg\n",
    "\n",
    "# dict_dist_to_trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### left over stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sel_iter = 0\n",
    "# sel_alt = 0\n",
    "\n",
    "# # show the traversed nodes and their costs\n",
    "# r = routes_matrix[sel_iter][sel_alt]\n",
    "# df_0 = df_node_lst[sel_iter][sel_alt]\n",
    "# df_f = df_0[df_0[\"osmid\"].isin(r)]\n",
    "# df_f[beta_features + [\"previous\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for checking purposes\n",
    "# all_edges[(all_edges.edge_id == (213581228, 128250, 2))].all_cost.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
