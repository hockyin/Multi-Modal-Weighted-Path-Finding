{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import math\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "from operator import itemgetter\n",
    "import os\n",
    "%matplotlib inline\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = 100\n",
    "\n",
    "# norm features of the model which are individual for each mode of transportation and used for the calculation of the uc\n",
    "norm_mod_features = [\"norm_travel_time\", \"norm_discomfort\", \"norm_price\"]\n",
    "\n",
    "# norm features of the model which are independet of the mode of transportation and used for the calculation of the uc\n",
    "norm_features = [\"norm_co2\"]\n",
    "\n",
    "# not normalized value of the features dependent of modality: used for initialization of normalized values\n",
    "nnm_features = [\"travel_time\", \"discomfort\", \"price\"]\n",
    "\n",
    "# not normalized value of the features independent of modality: used for initialization of none normalized values\n",
    "nn_features = [\"co2\"]\n",
    "\n",
    "# all mode of transportations\n",
    "mode = [\"car\", \"bike\", \"walk\", \"public\"]\n",
    "\n",
    "# all beta_features applying to normalized features\n",
    "beta_features = [\"norm_co2\", \"norm_num_trans\",\n",
    "                 \"norm_travel_time_car\", \"norm_travel_time_bike\", \"norm_travel_time_walk\", \"norm_travel_time_public\",\n",
    "                 \"norm_discomfort_car\", \"norm_discomfort_bike\", \"norm_discomfort_walk\", \"norm_discomfort_public\",\n",
    "                 \"norm_price_car\", \"norm_price_bike\", \"norm_price_walk\", \"norm_price_public\"]\n",
    "\n",
    "# file path for preprocessed dataframes\n",
    "try:\n",
    "    from src.a_star import prep_dataframes  #note: MMWPF path must be added to enviornmental variables PYTHONPATH (see setup.py)\n",
    "    prep_path = prep_dataframes.path()\n",
    "except:\n",
    "    prep_path = os.path.join(\"./\", \"prep_dataframes\")\n",
    "    if not os.path.exists(prep_path):\n",
    "        print('%s path does not exist!' % prep_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing edge and node network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.maps import graphML  #note: MMWPF path must be added to enviornmental variables PYTHONPATH (see setup.py)\n",
    "    composed_graph_path = graphML.path()\n",
    "except:\n",
    "    composed_graph_path = os.path.join(\"./\", \"comp_graph\")\n",
    "    if not os.path.exists(composed_graph_path):\n",
    "        print('%s path does not exist!' % composed_graph_path)\n",
    "\n",
    "# hardcoded path - needs to be adjusted\n",
    "g_drive_path = os.path.join(composed_graph_path, \"G_drive.graphml\")\n",
    "g_bike_path = os.path.join(composed_graph_path, \"G_bike.graphml\")\n",
    "g_walk_path = os.path.join(composed_graph_path, \"G_walk.graphml\")\n",
    "g_public_path = os.path.join(composed_graph_path, \"G_public.graphml\")\n",
    "g_all_path = os.path.join(composed_graph_path, \"G_all.graphml\")\n",
    "\n",
    "G_drive = ox.load_graphml(g_drive_path)\n",
    "G_bike = ox.load_graphml(g_bike_path)\n",
    "G_walk = ox.load_graphml(g_walk_path)\n",
    "G_public = ox.load_graphml(g_public_path)\n",
    "G_all = ox.load_graphml(g_all_path)\n",
    "G_all = ox.project_graph(G_all)\n",
    "\n",
    "lst_g = [G_drive, G_bike, G_walk, G_public]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_all = nx.compose(G_drive, G_bike)\n",
    "# G_all = nx.compose(G_all, G_walk)\n",
    "# G_all = nx.compose(G_all, G_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walking speed is way too high, need so scale it so it is between 3 - 10\n",
    "def rescale_walk_speed(spd):\n",
    "    if spd <= 10.0:\n",
    "        return spd\n",
    "    elif spd <= 20.0:\n",
    "        return spd / 2.0\n",
    "    elif spd <= 30.0:\n",
    "        return spd / 3.0\n",
    "    elif spd <= 40.0:\n",
    "        return spd / 4.0\n",
    "    elif spd <= 50.0:\n",
    "        return spd / 5.0\n",
    "    elif spd <= 60.0:\n",
    "        return spd / 6.0\n",
    "    elif spd <= 70.0:\n",
    "        return spd / 7.0\n",
    "    elif spd <= 80.0:\n",
    "        return spd / 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_nodes = []\n",
    "lst_edges = []\n",
    "\n",
    "# the order of the network is: 1. drive, 2. bike, 3. walk, 4. public\n",
    "for mode_idx, G in enumerate(lst_g):\n",
    "    # Reproject the data from a WGS84 format into a metric system \n",
    "    G_tmp_proj = ox.project_graph(G)\n",
    "    G_tmp_proj = ox.add_edge_speeds(G_tmp_proj)\n",
    "    G_tmp_proj = ox.add_edge_travel_times(G_tmp_proj)\n",
    "    g_node_tmp, g_edge_tmp = ox.graph_to_gdfs(G_tmp_proj)\n",
    "    # mode_idx 2 is walk mode\n",
    "    # only the speed of for walk needs to be adjusted\n",
    "    if mode_idx == 2:\n",
    "        g_edge_tmp[\"speed_kph\"] = g_edge_tmp[\"speed_kph\"].apply(rescale_walk_speed)\n",
    "        # recalculate travel time based on the new speed_kph\n",
    "        g_edge_tmp[\"travel_time\"] = g_edge_tmp[\"length\"] / (g_edge_tmp[\"speed_kph\"] * 1000 / 3600)\n",
    "    # mode_idx 3 is public mode\n",
    "    elif mode_idx == 3:\n",
    "        # drop column in order to concatenate them later on\n",
    "        g_node_tmp.drop([\"stopid\", \"color\"], axis=1, inplace=True)\n",
    "    # rescale speed to meter per second\n",
    "    g_edge_tmp[\"speed_ms\"] = g_edge_tmp[\"speed_kph\"] * 1000 / 3600\n",
    "    \n",
    "    ###### Comment the below code out, if your osmnx version is older than 1.0.1 ######\n",
    "    # unstack the multiindex\n",
    "#     g_node_tmp.drop(\"osmid\", axis=1, inplace=True)\n",
    "#     g_node_tmp =  g_node_tmp.reset_index()\n",
    "#     g_node_tmp.index = g_node_tmp.osmid\n",
    "#     g_node_tmp.index.rename('', inplace=True)\n",
    "    ###### Comment the above code out, if your osmnx version is older than 1.0.1 ######\n",
    "\n",
    "    g_edge_tmp = g_edge_tmp.reset_index()\n",
    "\n",
    "    lst_nodes.append(g_node_tmp)\n",
    "    lst_edges.append(g_edge_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliy function for edge transformation\n",
    "def indices(mylist, value):\n",
    "    return [i for i,x in enumerate(mylist) if x[1]==value]\n",
    "\n",
    "# if there are 2 same edges with different number than they are stored in a list\n",
    "def tran_mult_edges(lst):\n",
    "    # Some nodes don't have any other adjacent edges and returns a NaN value\n",
    "    # for those nodes just return NaN value\n",
    "    if lst != lst:\n",
    "        return lst\n",
    "    trg_lst = [l[1] for l in lst]\n",
    "    if len(set(trg_lst)) == len(trg_lst):\n",
    "        return lst\n",
    "    else:\n",
    "        for trg in set(trg_lst):\n",
    "            idc = indices(lst, trg)\n",
    "            # if no multiple edges for the target value then return\n",
    "            if len(idc) > 1:\n",
    "                first_idx = idc[0]\n",
    "                t = [lst[idx] for idx in idc]\n",
    "                for idx in sorted(idc, reverse = True):  \n",
    "                    del lst[idx]\n",
    "                lst.insert(first_idx, t)\n",
    "            else:\n",
    "                continue\n",
    "        return lst\n",
    "    \n",
    "def calc_discomfort(length, mode):\n",
    "    d = length\n",
    "    if mode == \"car\":\n",
    "        d = length\n",
    "    elif mode == \"bike\":\n",
    "        d = 2.0 * length\n",
    "    elif mode == \"walk\":\n",
    "        d = 3.0 * length\n",
    "    elif mode == \"public\":\n",
    "        d = 1.2 * length  \n",
    "    return d\n",
    "\n",
    "def calc_co2(length, mode):\n",
    "    # ref --> 127g / km\n",
    "    if mode == \"car\":\n",
    "        co2 = length * 127 / 1000\n",
    "    # 21g / km\n",
    "    elif mode == \"bike\":\n",
    "        co2 = length * 21 / 1000\n",
    "    # 5g / km\n",
    "    elif mode == \"walk\":\n",
    "        co2 = length * 5 / 1000\n",
    "    # bus = 75, ubahn = 30.5, train = 28 --> avg = 44.5 / km\n",
    "    elif mode == \"public\":\n",
    "        co2 = length * 39.125 / 1000 \n",
    "    return co2\n",
    "\n",
    "# public is calculated in a lambda function since it uses length instead of travel_time\n",
    "def calc_price(travel_time, mode):\n",
    "    # ref --> sixt share start at 0.09 per minute, depends on offer/demand --> assume in average a cost of 0.12 per minute\n",
    "    if mode == \"car\":\n",
    "        pr_cost = 0.002\n",
    "    elif mode == \"bike\":\n",
    "        pr_cost = 0.0013 \n",
    "    elif mode == \"walk\":\n",
    "        pr_cost = 0\n",
    "    # day ticket in cost per second: 7.9 per day\n",
    "    elif mode == \"public\":\n",
    "        pr_cost = 0.0001\n",
    "    # opportunity cost of 8â‚¬ / hour\n",
    "    op_cost = 8 / 3600\n",
    "    price = (op_cost + pr_cost) * travel_time\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ edge transformations ################\n",
    " \n",
    "def select_add_transform_attributes(frames, scaler):\n",
    "    filtered_columns = [\"osmid\", \"highway\", \"length\", \"speed_ms\", \"travel_time\", \"u\", \"v\", \"key\"]\n",
    "    # order of the list: [df_car, df_bike, df_walk]\n",
    "    # only select the specified columns\n",
    "#     lst_f = [f.loc[:,columns] for f in frames]\n",
    "    lst_f = [f.reindex(columns=filtered_columns, index=None) for f in frames]\n",
    "    \n",
    "    # store the index of each df in order to split them up again later on\n",
    "    lst_idx = []\n",
    "    next_idx = 0\n",
    "             \n",
    "    # add column for each cost category by transforming them into a dictionary\n",
    "    # where each dataframe has the mode of transportation as key\n",
    "    # increment count by one after each f to select the corresponding mode of transportation\n",
    "    # mode = [\"car\", \"bike\", \"walk\", public] --> e.g. mode[0] == car\n",
    "\n",
    "    for count, f in enumerate(lst_f):\n",
    "        f[\"co2\"] = f[\"length\"].apply(calc_co2, mode=mode[count]) \n",
    "        f[\"discomfort\"] = f[\"length\"].apply(calc_discomfort, mode=mode[count])\n",
    "        f[\"price\"] = f[\"travel_time\"].apply(calc_price, mode=mode[count])\n",
    "        # only used for initilization\n",
    "        for att in beta_features:\n",
    "            f[att] = 0.0\n",
    "        \n",
    "        # save the last index of each df\n",
    "        # after each iteration add the last index value to it in order to extract it later on \n",
    "        next_idx += f.index.stop\n",
    "        lst_idx.append(next_idx)\n",
    "    \n",
    "    # combine all frames for rescaling\n",
    "    combined_frames = pd.concat(lst_f)\n",
    "    combined_frames.reset_index(inplace=True)\n",
    "    # scale all values with the specified scaler\n",
    "    combined_frames[norm_features + norm_mod_features] = scaler.fit_transform(combined_frames[nn_features + nnm_features])\n",
    "    \n",
    "    # split up the df for each mode again after rescaling\n",
    "    f_car = combined_frames.iloc[:lst_idx[0], :].reset_index(drop=True)\n",
    "    f_bike = combined_frames.iloc[lst_idx[0]:lst_idx[1], :].reset_index(drop=True)\n",
    "    f_walk = combined_frames.iloc[lst_idx[1]:lst_idx[2], :].reset_index(drop=True)\n",
    "    f_public = combined_frames.iloc[lst_idx[2]:lst_idx[3], :].reset_index(drop=True)\n",
    "    lst_f = [f_car, f_bike, f_walk, f_public]\n",
    "    \n",
    "    for count, f in enumerate(lst_f):\n",
    "        \n",
    "        # transform combine all cost attributes into one column as a tuple first\n",
    "        # then transform the tuples into a dict\n",
    "        f[\"all_cost\"] = f.apply(lambda x: (-x[\"norm_co2\"], -x[\"norm_travel_time\"], -x[\"norm_discomfort\"], -x[\"norm_price\"]), axis=1)\n",
    "        f[\"all_cost\"] = f[\"all_cost\"].apply(lambda t: {mode[count]: {\"norm_co2\": t[0],\n",
    "                                                                     \"norm_travel_time\": t[1],\n",
    "                                                                     \"norm_discomfort\": t[2],\n",
    "                                                                     \"norm_price\": t[3]\n",
    "                                                                     }})\n",
    "        f[\"true_cost\"] = f.apply(lambda x: (-x[\"co2\"], -x[\"travel_time\"], -x[\"discomfort\"], -x[\"price\"]), axis=1)\n",
    "        f[\"true_cost\"] = f[\"true_cost\"].apply(lambda t: {mode[count]: {\"co2\": t[0],\n",
    "                                                                       \"travel_time\": t[1],\n",
    "                                                                       \"discomfort\": t[2],\n",
    "                                                                       \"price\": t[3]\n",
    "                                                                       }})\n",
    "        # add rescaled speed for each mode of transportation                                                             \n",
    "        f[\"speed_ms\"] = f[\"speed_ms\"].apply(lambda spd: {mode[count]: spd})\n",
    "        \n",
    "        f.drop(beta_features + nn_features + nnm_features + norm_mod_features, axis=1, inplace=True)\n",
    "    return lst_f\n",
    "        \n",
    "def combine_edges(frames):\n",
    "    edges_drive, edges_bike, edges_walk, edges_public = frames[0], frames[1], frames[2], frames[3]\n",
    "    # unique identifier for an edge --> osmid is not unique for each edge\n",
    "    keys = [\"u\", \"v\", \"key\"]\n",
    "    att = \"all_cost\"\n",
    "    true_att = \"true_cost\"\n",
    "    spd_att = \"speed_ms\"\n",
    "    col_idx = len(frames[0].columns)\n",
    "\n",
    "    # cars & bike --> not null values of the left join\n",
    "    # only cars --> null values of the left join\n",
    "    all_edges = pd.merge(edges_drive, edges_bike, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get all edges which are available for cars and bikes \n",
    "    r = all_edges[pd.notnull(all_edges[\"osmid_right\"])]\n",
    "\n",
    "    # for all edges which are also bike routes the bike dictionary will be appended to the current dict\n",
    "    [r.loc[x, att].update(r.loc[x, att + \"_right\"]) for x in r.index]\n",
    "    [r.loc[x, true_att].update(r.loc[x, true_att + \"_right\"]) for x in r.index]\n",
    "    [r.loc[x, spd_att].update(r.loc[x, spd_att + \"_right\"]) for x in r.index]                                                                   \n",
    "\n",
    "    # cars & bike --> not null values of the left join\n",
    "    # only bikes --> null values of the left join\n",
    "    only_bike = pd.merge(edges_bike, edges_drive, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get alle edges which are only available for bikes\n",
    "    only_bike = only_bike[only_bike[\"osmid_right\"].isnull()].copy()\n",
    "\n",
    "    # combine the base edges with the bike only edges\n",
    "    all_edges = pd.concat([all_edges, only_bike]).iloc[:, :col_idx]\n",
    "    \n",
    "    # available for walk --> not null values of the left join\n",
    "    # not available for walk --> null values of the left join\n",
    "    all_edges_plus_walk = pd.merge(all_edges, edges_walk, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get all edges which are available for walk \n",
    "    w = all_edges_plus_walk[pd.notnull(all_edges_plus_walk[\"osmid_right\"])]\n",
    "\n",
    "    # for all edges which are also bike routes the bike dictionary will be appended to the current dict\n",
    "    [w.loc[x, att].update(w.loc[x, att + \"_right\"]) for x in w.index]\n",
    "    [w.loc[x, true_att].update(w.loc[x, true_att + \"_right\"]) for x in w.index]\n",
    "    [w.loc[x, spd_att].update(w.loc[x, spd_att + \"_right\"]) for x in w.index]\n",
    "\n",
    "    # only walks --> null values of the left join\n",
    "    only_walk = pd.merge(edges_walk, all_edges, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get alle edges which are only available for walks\n",
    "    only_walk = only_walk[only_walk[\"osmid_right\"].isnull()].copy()\n",
    "\n",
    "    # combine the base edges with the walk only edges\n",
    "    all_edges_plus_walk = pd.concat([all_edges_plus_walk, only_walk]).iloc[:, :col_idx]\n",
    "    \n",
    "    # available for public --> not null values of the left join\n",
    "    # not available for public --> null values of the left join\n",
    "    all_edges_plus_public = pd.merge(all_edges_plus_walk, edges_public, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get all edges which are available for public \n",
    "    p = all_edges_plus_public[pd.notnull(all_edges_plus_public[\"osmid_right\"])]\n",
    "\n",
    "    # for all edges which are also public routes the public dictionary will be appended to the current dict\n",
    "    [p.loc[x, att].update(p.loc[x, att + \"_right\"]) for x in p.index]\n",
    "    [p.loc[x, true_att].update(p.loc[x, true_att + \"_right\"]) for x in p.index]\n",
    "    [p.loc[x, spd_att].update(p.loc[x, spd_att + \"_right\"]) for x in p.index]\n",
    "    # only public --> null values of the left join\n",
    "    only_public = pd.merge(edges_public, all_edges_plus_walk, how=\"left\", on=keys, suffixes=(\"\", \"_right\"))\n",
    "\n",
    "    # get alle edges which are only available for public\n",
    "    only_public = only_public[only_public[\"osmid_right\"].isnull()].copy()\n",
    "#     print(\"before public\", all_edges_plus_walk.shape)\n",
    "#     print(\"Both\", p.shape)\n",
    "#     print(\"Only public\", only_public.shape)\n",
    "\n",
    "    # combine the base edges with the public only edges\n",
    "    all_edges_plus_public = pd.concat([all_edges_plus_public, only_public]).iloc[:, :col_idx]\n",
    "    return all_edges_plus_public.reset_index(drop=True)    \n",
    "\n",
    "def full_transform_edges(frames, scaler):\n",
    "    df_edges = combine_edges(select_add_transform_attributes(frames, scaler))\n",
    "    df_edges[\"edge_id\"] = df_edges[\"u\"].astype(str) + \",\" + df_edges[\"v\"].astype(str) + \",\" + df_edges[\"key\"].astype(str)\n",
    "    df_edges[\"edge_id\"] = df_edges[\"edge_id\"].apply(lambda s: tuple([int(k) for k in s.split(\",\")]))\n",
    "    return df_edges\n",
    "\n",
    "################ node transformations ################\n",
    "\n",
    "# initiate all nodes with maximum pathcost, not explored and no previous node\n",
    "def initiate_nodes(df_nodes):\n",
    "    df_nodes[\"f_cost\"] = sys.maxsize\n",
    "    df_nodes[\"path_cost\"] = 0.0\n",
    "    df_nodes[\"heuristic\"] = 0.0\n",
    "    df_nodes[\"num_trans\"] = 0.0\n",
    "    df_nodes[\"norm_num_trans\"] = 0.0\n",
    "    df_nodes[\"max_trans\"] = 0.0\n",
    "    df_nodes[\"previous\"] = None\n",
    "    df_nodes[\"explored\"] = False\n",
    "    df_nodes[\"frontier\"] = False\n",
    "    df_nodes[\"trans_mode\"] = None\n",
    "    df_nodes[\"edge_idx\"] = None\n",
    "    for f in nnm_features:\n",
    "        for m in mode:\n",
    "            df_nodes[\"norm\" + \"_\" + f + \"_\" + m] = 0.0\n",
    "            df_nodes[f + \"_\" + m] = 0.0       \n",
    "    for nn_f in nn_features:\n",
    "        df_nodes[\"norm\" + \"_\" + nn_f] = 0.0\n",
    "        df_nodes[nn_f] = 0.0\n",
    "\n",
    "\n",
    "# for each vertex \"u\" add every edge which starts from \"u\" to an adjacent list \n",
    "def add_adjacent(df_nodes, df_edges):\n",
    "    adj = pd.merge(df_nodes, df_edges, left_on=\"osmid\", right_on=\"u\", suffixes=(\"_node\", \"_edge\"))\n",
    "    # each edge_id is added to the adjacent list of the node\n",
    "    adj_edges = adj.groupby(\"osmid_node\")[\"edge_id\"].apply(lambda x: list(x))\n",
    "    df_nodes[\"adjacent\"] = adj_edges.copy()\n",
    "    df_nodes[\"adjacent\"] = df_nodes[\"adjacent\"].apply(lambda x: tran_mult_edges(x))\n",
    "        \n",
    "def full_transform_nodes(frames, df_edges):\n",
    "    for f in frames:\n",
    "        f.drop([\"highway\", \"ref\"], axis=1, inplace=True, errors='ignore')\n",
    "    df_c = pd.concat(frames)\n",
    "    df_c.drop_duplicates(inplace=True)\n",
    "    initiate_nodes(df_c)\n",
    "    add_adjacent(df_c, df_edges)\n",
    "    return df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "all_edges = full_transform_edges(lst_edges, scaler)\n",
    "all_edges.drop(\"index\", axis=1, inplace=True)\n",
    "all_edges.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = full_transform_nodes(lst_nodes, all_edges)\n",
    "all_nodes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check edge and node network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_nodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at the all_cost dictionary\n",
    "all_edges[\"all_cost\"].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take a look at the all_cost dictionary\n",
    "all_edges[\"true_cost\"].iloc[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting target ids which are contained in all networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_sel_trg_id = []\n",
    "# list of list of ids for each network\n",
    "lst_ids_mode =  [list(nx.get_node_attributes(g, \"osmid\").values()) for g in lst_g]\n",
    "# list of all node ids in the entire network\n",
    "lst_id = list(all_nodes.osmid)\n",
    "\n",
    "# length equals to number of observations\n",
    "while len(lst_sel_trg_id) < num_obs:\n",
    "    rnd_gen_base = 60275\n",
    "    rnd_seeds = [i*rnd_gen_base for i in range(1,num_obs+1)]\n",
    "    for seed in rnd_seeds:\n",
    "        exist = True\n",
    "        np.random.seed(seed)\n",
    "        trg_id = lst_id.pop(np.random.randint(1, len(lst_id)))\n",
    "        # check that the trg_id is in every network to minimize the chance that a node can not be reached\n",
    "        for ids_mode in lst_ids_mode:\n",
    "            if trg_id not in ids_mode:\n",
    "                exist = False\n",
    "                break\n",
    "        if exist:\n",
    "            lst_sel_trg_id.append(trg_id)\n",
    "    rnd_gen_base += 1\n",
    "\n",
    "# for g_node in lst_nodes\n",
    "\n",
    "lst_sel_trg_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_heuristic(src_id, trg_id):\n",
    "    src_lat, src_lon = all_nodes.loc[src_id, [\"lat\", \"lon\"]]\n",
    "    trg_lat, trg_lon = all_nodes.loc[trg_id, [\"lat\", \"lon\"]]\n",
    "    dist_to_trg = ox.distance.great_circle_vec(src_lat, src_lon, trg_lat, trg_lon, earth_radius=6371009)\n",
    "    return dist_to_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the built in dijsktra algorithm to calculate the shortest distance to each node\n",
    "# lst_sel_trg_id: list containing all trg ids\n",
    "# G_all: Multigraph containing the whole network\n",
    "\n",
    "# dict of dictionaries containing the distance of each src_node to each target node\n",
    "lst_dist_to_trg = []\n",
    "lst_id = list(nx.get_node_attributes(G_all, \"osmid\").values())\n",
    "\n",
    "for count, trg_id in enumerate(lst_sel_trg_id):\n",
    "    clear_output(wait=True)\n",
    "    dist_to_trg = {}\n",
    "    # calculate the shortest great-circle distance of each src_id to the trg_id\n",
    "    # it is always underapproximative since it is the shortest distance possible to the trg_id\n",
    "    for src_id in lst_id:\n",
    "        shortest_dist = dist_heuristic(src_id, trg_id)\n",
    "        dist_to_trg[src_id] = shortest_dist\n",
    "    print(f\"Remaining number of trg_ids: {len(lst_sel_trg_id) - (count+1)}\")\n",
    "    lst_dist_to_trg.append(dist_to_trg)\n",
    "\n",
    "lst_dist_to_trg;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lst_g as pickle file\n",
    "g_file_name = os.path.join(prep_path, \"lst_g4_100\")\n",
    "try: \n",
    "    scaler_file = open(g_file_name, 'wb') \n",
    "    pickle.dump(lst_g, scaler_file) \n",
    "    scaler_file.close() \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler as pickle file\n",
    "mm_file_name = os.path.join(prep_path, \"min_max4_100\")\n",
    "try: \n",
    "    scaler_file = open(mm_file_name, 'wb') \n",
    "    pickle.dump(scaler, scaler_file) \n",
    "    scaler_file.close() \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save edge and node network as pickle files\n",
    "node_path = os.path.join(prep_path, \"all_nodes.pkl\")\n",
    "edge_path = os.path.join(prep_path, \"all_edges.pkl\")\n",
    "\n",
    "if not os.path.exists(prep_path):\n",
    "    os.makedirs(prep_path)\n",
    "    \n",
    "all_nodes.to_pickle(node_path)\n",
    "all_edges.to_pickle(edge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save precalculated dict to file\n",
    "trg_file_name = os.path.join(prep_path, \"trg4_100\")\n",
    "try: \n",
    "    geeky_file = open(trg_file_name, 'wb') \n",
    "    pickle.dump(lst_sel_trg_id, geeky_file) \n",
    "    geeky_file.close() \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save precalculated dict to file\n",
    "dict_file_name = os.path.join(prep_path, \"dist_to_trg4_100\")\n",
    "try: \n",
    "    geeky_file = open(dict_file_name, 'wb') \n",
    "    pickle.dump(lst_dist_to_trg, geeky_file) \n",
    "    geeky_file.close() \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save G_all\n",
    "# g_all_file_name = os.path.join(composed_graph_path, \"G_all.graphml\")\n",
    "# ox.io.save_graphml(G_all, g_all_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
